# -*- coding: utf-8 -*-
"""toxic_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19SSiXDrG9PviJg7KLsWtpLe-DfE_5G3v
"""

# Importing important libraries
import numpy as np
import pandas as pd
from sklearn import metrics
import transformers
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from tqdm import tqdm
import matplotlib.pyplot as plt

# Setting up the device for GPU usage
from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'

train = pd.read_csv('../data/toxic/train/train.csv') 
val = pd.read_csv('../data/toxic/val/val.csv')

# setting target labels in one column for Custom Dataset Class
train['labels'] = train[train.columns[1:]].values.tolist()
train = train[['comment_text', 'labels']].copy()
train.head()

val['labels'] = val[val.columns[1:]].values.tolist()
val = val[['comment_text', 'labels']].copy()
val.head()

print(f"Train Dataset Shape: {train.shape}")
print(f"Validation Dataset Shape: {val.shape}")

# Defining some key variables required for training
MAX_LEN = 200
TRAIN_BATCH_SIZE = 32
VALID_BATCH_SIZE = 16
EPOCHS = 1
LEARNING_RATE = 1e-05
tokenizer = BertTokenizer.from_pretrained('../models/pretrained/bert-base-uncased')

# Creating Dataset and DataLoader for neural net
class DetoxDataset(Dataset):

    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.comment_text = dataframe.comment_text
        self.targets = self.data.labels
        self.max_len = max_len

    def __len__(self):
        return len(self.comment_text)

    def __getitem__(self, index):
        comment_text = str(self.comment_text[index])
        comment_text = " ".join(comment_text.split())

        inputs = self.tokenizer.encode_plus(
            comment_text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True,
            truncation=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]


        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }

train_set = DetoxDataset(train, tokenizer, MAX_LEN)
val_set = DetoxDataset(val, tokenizer, MAX_LEN)

train_params = {'batch_size': TRAIN_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
               }

val_params = {'batch_size': VALID_BATCH_SIZE,
              'shuffle': True,
              'num_workers': 0
             }

train_loader = DataLoader(train_set, **train_params)
val_loader = DataLoader(val_set, **val_params)

# creating and fine tuning customized model on top of BERT
class DetoxClass(torch.nn.Module):
    def __init__(self):
        super(DetoxClass, self).__init__()
        self.l1 = BertModel.from_pretrained('../models/pretrained/bert-base-uncased')
        self.l2 = torch.nn.Dropout(0.3)
        self.l3 = torch.nn.Linear(768, 6)
    
    def forward(self, ids, mask, token_type_ids):
        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        
        return output

model = DetoxClass()
model.to(device)

# defining loss function
def loss_fn(outputs, targets):
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)

# setting optimizer
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)

# fine tuning the model
def train(epochs):
    losses = []
    for epoch in range(epochs):
        model.train()
        with tqdm(train_loader, unit = "batch") as tepoch:
            for _, data in enumerate(tepoch, 0):
                tepoch.set_description(f"Epoch {epoch + 1}")
                
                ids = data['ids'].to(device, dtype = torch.long)
                mask = data['mask'].to(device, dtype = torch.long)
                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
                targets = data['targets'].to(device, dtype = torch.float)

                outputs = model(ids, mask, token_type_ids)

                loss = loss_fn(outputs, targets)
                losses.append(loss.item())
                tepoch.set_postfix(loss=loss.item())
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
    return losses

losses = train(EPOCHS)

# plotting learning curve
plt.figure(figsize=(7,6))
plt.plot(range(1, 3991), losses)
plt.title('Learning Curve')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.show()

# validating the model on different metrics
def validation(epoch):
    model.eval()
    fin_targets=[]
    fin_outputs=[]
    with torch.no_grad():
        with tqdm(val_loader, unit = "batch") as tepoch:
            for _, data in enumerate(tepoch, 0):
                ids = data['ids'].to(device, dtype = torch.long)
                mask = data['mask'].to(device, dtype = torch.long)
                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
                targets = data['targets'].to(device, dtype = torch.float)
                outputs = model(ids, mask, token_type_ids)
                fin_targets.extend(targets.cpu().detach().numpy().tolist())
                fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
                
    return fin_outputs, fin_targets

for epoch in range(EPOCHS):
    outputs, targets = validation(epoch)
    outputs = np.array(outputs) >= 0.5
    accuracy = metrics.accuracy_score(targets, outputs)
    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')
    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')
    print(f"Accuracy Score = {accuracy}")
    print(f"F1 Score (Micro) = {f1_score_micro}")
    print(f"F1 Score (Macro) = {f1_score_macro}")

# saving the trained model for inference
PATH = '../models/fine_tuned/toxic_model.pth'

torch.save(model.state_dict(), PATH)